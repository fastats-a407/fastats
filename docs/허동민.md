# 1일차

- 읽어 본 자료 :

[엘라스틱서치(Elasticsearch)에서 관계형 데이터 모델링하기 | 인사이트리포트 | 삼성SDS](https://www.samsungsds.com/kr/insights/elastic_data_modeling.html)

- 느낀 점 : 문서 기반 데이터베이스인 엘라스틱서치는 NoSQL의 일종으로, 삭제 수정과 같은 DML은 지양해야한다. DML이 필요할 때는 RDB를 활용하는 것을 추천. 태생이 검색엔진이므로 **데이터 모델링**이 가장 중요함! **Denormalization 모델링**이 가장 인상 깊었음.
- 다음 찾아볼 자료 : Denormalization 모델링의 방법을 더 조사

# 2일차

- ElasticSearch : 인덱싱한 데이터가 있는 DB라고 생각
- LogStash : RDB에 있는 데이터를 ElasticSearch로 적재시키는 역할
- Kibana : ElasticSearch에 적재된 데이터를 시각적으로 확인할 있는 GUI
- 데이터 확인하는 방법 : [localhost:5601](http://localhost:5601)(ElasticSearch 포트) → http://elasticsearch:9200을 연결(안된다면 https로 시도) → docker-compose에 있는 ./kibana.yml에 적힌 username과 passwor에 맞게 GUI 접속 시도 → 왼쪽 탭의 Management → Dev Tools에 들어가 적절한 명령어 사용
- ex)

```java
GET /board_index/_search
{
  "query": {
  "match_all": {}
}
}
# board_index : logstash.conf에서 설정한 인덱스명
```

# 3일차

- 엘라스틱 서치는 기본적으로 최대 10000개의 데이터까지만 표출가능하므로 Scroll API나 Search After를 활용하여 초과되는 데이터에 대한 접근이 필요하다.
- 기본적인 GET 검색의 경우 **_score** 필드 내림차순으로 정렬하는데 위 필드의 기준은 검색어와의 유사도를 기준으로 매김
- 필드(RDB의 칼럼)의 타입을 TEXT로 하면 ngram을 통해 부분문자열을 통한 검색을 지원하지만 KEYWORD로 지정할 경우 정확히 일치하는 경우만 검색 지원. wildcard라는 전략을 사용하여 일부 부분문자열을 포함시킬 수는 있음.

# 4일차

```java
    public static void main(String[] args) {
  String jdbcUrl = "jdbc:mysql://localhost:3306/mydatabase";
  String jdbcUser = "root";
  String jdbcPassword = "example";
  String elasticsearchHost = "http://localhost:9200";

  // Initialize JDBC connection
  try (Connection connection = DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword);
       Statement statement = connection.createStatement();
       RestHighLevelClient client = new RestHighLevelClient(
               RestClient.builder(new HttpHost("localhost", 9200, "http")))) {

    // Execute SQL query
    String sql = "SELECT * FROM board";
    ResultSet resultSet = statement.executeQuery(sql);

    // Process results and insert into Elasticsearch
    while (resultSet.next()) {
      String id = resultSet.getString("id");
      int memberId = resultSet.getInt("memberId");
      String title = resultSet.getString("title");
      String content = resultSet.getString("content");
      String type = resultSet.getString("type");
      int views = resultSet.getInt("views");
      int likes = resultSet.getInt("likes");

      // Create JSON document to insert into Elasticsearch
      String json = String.format(
              "{\"id\": \"%s\", \"memberId\": %d, \"title\": \"%s\", \"content\": \"%s\", \"type\": \"%s\", \"views\": %d, \"likes\": %d}",
              id, memberId, title, content, type, views, likes);

      IndexRequest request = new IndexRequest("board_index")
              .id(id)
              .source(json, XContentType.JSON);

      // Insert document into Elasticsearch
      client.index(request, RequestOptions.DEFAULT);
    }
  } catch (SQLException e) {
    e.printStackTrace();
  } catch (Exception e) {
    e.printStackTrace();
  }
}
```

를 참고하여 logstash대신 jdbc connector를 활용하여 MySQL → Elastic Search 적재 구현해보기.

# 5일차

- Bulk 처리 : 여러 문서에 대한 요청을 한번에 처리하는 것으로 네트워크의 오버헤드 부담을 줄일 수 있다.
  - 장점 : 네트워크 비용 절감 / 효율적인 리소스 사용
  - 단점 : 메모리 사용 증가 → 최적의 Bulk 메모리 요청 크기를 정해야함, 많은 양의 데이터를 한번에 처리하므로 예외처리에 각별한 신경
- IndexRequest에 Document로 지정한 Dto를 Json화하여 BulkRequest에 add를 하고 이를 BulkResponse에서 병렬 처리하는 형식으로 진행

# 6일차

- ES Repository 사용 시

![image.png](/docs/img/허동민/image01.png)

![image.png](/docs/img/허동민/image02.png)
- Bulk Request 사용 시

![image.png](/docs/img/허동민/image03.png)

![image.png](/docs/img/허동민/image04.png)

- 소규모 데이터에 관해서는 큰 차이를 보이지 않고 있다. 따라서 프로시저를 활용하여 대규모 데이터를 생성 후 테스트

```sql
USE fastats;

DELIMITER //

CREATE PROCEDURE insert_random_data(IN count INT)
BEGIN
    DECLARE i INT DEFAULT 0;
    DECLARE random_sector_id INT;
    DECLARE random_org_id INT;
    DECLARE random_survey_id INT;
    DECLARE random_stat_table_id BIGINT;

    -- sector 테이블에 랜덤 데이터 삽입
    WHILE i < count
        DO
            INSERT INTO sector (code, description)
            VALUES (CONCAT('A', LPAD(FLOOR(RAND() * 1000), 3, '0')),
                    CONCAT('Sector ', CHAR(FLOOR(65 + RAND() * 26))));
            SET i = i + 1;
END WHILE;

    SET i = 0;

    -- stat_org 테이블에 랜덤 데이터 삽입
    WHILE i < count
        DO
            INSERT INTO stat_org (code, name)
            VALUES (FLOOR(RAND() * 1000),
                    CONCAT('Organization ', CHAR(FLOOR(65 + RAND() * 26))));
            SET i = i + 1;
END WHILE;

    SET i = 0;

    -- stat_survey 테이블에 랜덤 데이터 삽입
    WHILE i < count
        DO
            SET random_sector_id = (SELECT id FROM sector ORDER BY RAND() LIMIT 1);
            SET random_org_id = (SELECT id FROM stat_org ORDER BY RAND() LIMIT 1);

INSERT INTO stat_survey (sector_id, org_id, name)
VALUES (random_sector_id, random_org_id,
        CONCAT('Survey ', CHAR(FLOOR(65 + RAND() * 26)), FLOOR(RAND() * 100)));
SET i = i + 1;
END WHILE;

    SET i = 0;

    -- stat_table 테이블에 랜덤 데이터 삽입
    WHILE i < count
        DO
            SET random_survey_id = (SELECT id FROM stat_survey ORDER BY RAND() LIMIT 1);

INSERT INTO stat_table (survey_id, name, content, comment, kosis_tb_id, kosis_view_link)
VALUES (random_survey_id,
        CONCAT('Table ', CHAR(FLOOR(65 + RAND() * 26))),
        'Random Text for Content',
        'Random Text for Comment',
        CONCAT('TB', LPAD(FLOOR(RAND() * 1000), 3, '0')),
        CONCAT('httpexamplecomtable', FLOOR(RAND() * 100)));
SET i = i + 1;
END WHILE;

    SET i = 0;

    -- coll_info 테이블에 랜덤 데이터 삽입
    WHILE i < count
        DO
            SET random_stat_table_id = (SELECT id FROM stat_table ORDER BY RAND() LIMIT 1);
INSERT INTO coll_info (stat_table_id, start_date, end_date, period)
VALUES (random_stat_table_id,
        "20220101", "20241231",
        CASE FLOOR(RAND() * 3)
          WHEN 0 THEN '1Y'
          WHEN 1 THEN '1Q'
          ELSE '1M'
          END);
SET i = i + 1;
END WHILE;
END //

DELIMITER ;

```

- 프로시저 생성 시 트러블슈팅 :
  - Date 타입인줄알고 열심히 Date 변환을 하였지만 알고보니 varchar(8)이였습니다.
  - Link의 경우 “ : / . “와 같은 특수 기호가 들어가 해당 기호에 대한 처리가 필요했습니다.
  - 4중 조인 테이블이였기에 순서를 먼저 고려하고 FK를 가져왔습니다.

![image.png](/docs/img/허동민/image05.png)

![image.png](/docs/img/허동민/image06.png)

- Bulk API로 병렬 저장시 생기는 문제로 메모리가 감당할 수 있는 수준을 넘어서면 저장하는데에 저장이 멈춘다.
  - 멀티스레드 사용하여 메모리가 감당할 수 있게 분산 처리한다.

![image.png](/docs/img/허동민/image07.png)

![image.png](/docs/img/허동민/image08.png)

# 7일차

| 개념 | 역할 | 기능 및 구성 | 비교 대상 |
| --- | --- | --- | --- |
| 데이터베이스(DB) | 전체 데이터를 관리하는 최상위 개념 | 여러 인덱스로 구성된 데이터 그룹 | RDBMS의 DB |
| 노드(Node) | 데이터 저장 및 검색 처리하는 서버 단위 | 여러 샤드를 저장하는 클러스터의 개별 서버 | RDBMS의 서버 |
| 샤드(Shard) | 데이터를 쪼개어 분산 저장하는 물리적 저장 단위 | 프라이머리 및 복제본 샤드로 구분 | RDBMS의 파티션 |
- ES DB에 Date 타입 저장하는 방법 :
  - String으로 명시해도 되는 이유로는 Spring Data ElasticSearch에서 알아서 매핑을 해준다~
  - format = {} 명시 후 pattern을 선언해주면 format = DateFormat.Custom과 같은 역할을 할 수 있다.
  - pattern 외의 형식이 들어올 경우 데이터가 안 들어갈 수 있다. pattern은 배열 형태로도 가능함.

```
@Field(type = FieldType.Date, format = {}, pattern = "yyyyMMdd")
private String collInfoStartDate;  // 수록 시작시기
```
# 8일차

- @Field(type = FieldType.Text, analyzer = "nori")를 Document 어노테이션으로 달면 Elastic Search에 Nori Plugin이 설치되었다면 별도의 config 설정없이 사용가능.
- @Field(type = FieldType.Text, analyzer = "커스텀한_Analyzer_이름")으로 커스터마이징한 분석기를 추가할 수 있음

# 9일차

- 오늘은 docker exec -it elasticsearch /bin/bash 의 명령어를 정리
  1. 존재하는 인덱스 확인

    ```sql
    curl -X GET "localhost:9200/_cat/indices?v"
    ```

  1. 인덱스 내 데이터 조회

    ```sql
    curl -X GET "localhost:9200/stat_data_index/_search?pretty"
    ```

  1. 인덱스 내 데이터의 카운트 조회

    ```sql
    curl -X GET "localhost:9200/stat_data_index/_count"
    ```

  1. 인덱스 삭제

    ```sql
    curl -X DELETE "localhost:9200/stat_data_index"
    ```