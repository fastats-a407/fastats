# 1일차(10.28)
- 1회차 강의
  - 뭐 없음
- 2회차 강의
  - 생성형 AI 개요
    - ML 종류
      - 지도학습(Supervised Learning)
        - 정답을 알고 있다고 가정
        - 학습을 위한 입력데이터와 정답에 해당하는 레이블 데이터 두가지가 쌍을 이룸
        - 분류(Classification)
          - 이산적 분류, 0인가? 1인가? 2인가?
        - 회귀(Refression)
          - 연속된 숫자, 주가 환율 가격 등 연속성
      - 비지도학습
        - 정답을 우리도 모르는 경우
          - 계속된 학습을 통해서 군집과 비슷한 모델을 생성
        - 군집화(Clustering)
      - 강화학습(Reinforcement Learning)
        - trial and error(시행착오) 를 통해서 스스로 만들어 낸 데이터를 통해서 학습
    - AI의 역사
      - transformer 짱짱
        - 자료를 받아 분포를 만드는 encoder와 자료를 받아서 분포에 해당하는 값을 내보내는 decoder로 나뉨
          - decoder를 사용해서 chatGPT가 나옴
    - 생성 모델의 역사
      - 넘어가요~
    - 생성 모델이란?
      - 실제 존재하지는 않지만 있을 법한 이미지(언어, 생산물)를 생성할 수 있는 모델
      - 분류 모델
        - 결정 경계를 학습
      - 생성 모델
        - 각 클래스의 분포를 학습
        - 다르게 말하자면 특정 모델에 특정 값을 넣으면 특정 분포에 해당한다 하면 특정 분포의 값을 보여주는 것
      - Training data가 주어졌을 때, 이 data가 가지는 real 분포와 같은 분포에서 sampling된 값으로 new data를 생성하는 모델
      - 생성 모델은 결합 확률을 학습
- 3회차 강의
  - NLP(Natural Language Processing) 자연어 처리의 발전과정
    - 중요한 내용 = Transformer 모델이 나온 이후 많이 바뀜/ 이전에는 RNN 모델(seq-to-seq)이 득세
    - 더 parameter들이 많아지고 커진다
  - 단어 임베딩 (Embedding)
    - 숫자화 된 단어의 나열로부터 sentiment 추출
    - 연관성 있는 단어들을 군집화하여 multi-dimension 공간에 vector로 표시
      - 단어나 문장을 vector space로 끼워 넣음
    - 예로, 호감/ 비호감 두가지의 label에 까라 관련 단어들을 두개의 category로 군집화
  - Transformer 모델
    - RNN 또는 CNN을 이용하여 sequence-align 하지 않은 최초의 self-attention model
      - 동의어, 순차 처리를 하지 않은 self-attention model
    - 기존의 encoder-decoder 구조를 유지
    - RNN을 제거함으로 병렬처리 가능 (GPU 효율 극대화)
  - Transformer Architecture
    - 시작은 번역기로 시작함
    - Encoder Block
      - 통과 할 때마다 다양한 값이 나옴
      - 그것을 Decoder Block에 넣고 가장 높은 확률의 단어가 나오는 것
    - Positional Encoding
      - 병렬적으로 처리되는 단어에 위치 정보를 추가해주는 것
    - Self-Attention layer
      - Feed Forward와 Multi-Head Attention으로 구현
        - Feed Forward는 일종의 dense layer
      - Masked Multi-Head Attention
        - 뒷부분을 먼저 보지 않게 하려고 mask를 씌운 것
  - Self-Attention (Intra-Attention)
    - Attention을 자기 자신에 대해 수행
      - 문장 내 단어들 간의 유사도를 구함
    - Self-attention 계산
      - 3개의 vector 필요
        - Query Vector
        - Key Vector
        - Value Vector
  - Positional Encoding Example
    - RNN과 달리 병렬적 처리
      - 단어의 순서를 감지 못함
      - positional encoding을 통해 따로 벡터값을 처리함
      - sin, cos과 같은 함수를 이용해서 값을 다시 섞어줌
    - Transformer는 수학적 방식(삼각함수)을 사용했지만 BERT는 이마저도 모델에 학습시킴
  - Multi-Head Attention
    - 여러개의 attention을 병렬로 사용한 후 Attention Head를 연결
      - 다른 시각으로 단어간의 상관관계 파악
    - 각각이 random 하게 초기화 되므로 training 후 다른 subspace 표시
  - Transformer 파생 모델
    - Only Encoder
      - BERT
    - Encoder Decoder
      - TS
      - BART
    - Only Decoder
      - GPT
      - BLOOM
      - Jurassic
      - LLaMA
- 4회차 강의
  - GPT?
    - Transformer의 decoder 부분을 가능한 높이 쌓은 구성(96 layer)
    - 나머지는 자랑 내용
  - GPT : Few shot learnking
    - 지도학습
      - CNN model의 학습 방법
      - 다량의 데이터 필요
    - Zero shot learning
      - 비슷하지만 다른 학습으로 한번에 구분
      - 예) 말과 젖소의 사진만으로 학습 > 얼룩말은 젖소의 색을 가진 말 > Model은 얼룩말 사진을 구분
    - One shot learning
      - 한번의 a와 관련된 학습으로 a'을 구분
    - Few shot learning
      - 몇개의 데이터를 보여줌
      - model은 데이터를 구분 가능해짐
  - 언어 생성 모델의 환각 효과 (Hallucination Effect)
    - LLM의 가장 큰 문제점, 한계점
    - 모델이 사실이 아닌 정보, 논리적으로 맞지 않는 내용을 생성하는 것
    - 충분한 데이터를 얻지 못하거나 정보가 부족할 때 발생
    - 해결책
      - 잘하면 됩니다
    - 발생원인
      - 아무도 모름
  - Autoregressive model
    - Decoder only model임
    - `자동 회귀 모델` 입력 시퀀스를 하나씩 반복하여 이전 토큰 시퀀스를 기반으로 다음 토큰을 예측
    - 수많은 예시로 다음 토큰 예측 방법 학습, 모델은 통계적 표현을 구축
    - Zero shot 추론 능력을 보여줌
  - How to generate language?
    - 이전 time step의 결과를 다음 time step의 input으로 feed 하고, 각 step에서 가장 높은 확률의 다음 단어를 선택(greedy selection) 하거나 혹은 확률 분포에 따라 sampling
  - Decoding Strategy
    - Greedy 전략
      - 확률 분포 중 가장 높은 확률 선택
      - 언제나 같은 결과가 나옴
    - Sampling 전략
      - 분포 확률에 따라 Random Sampling
      - 매번 번역이 바뀔 수 있음
    - Beam-Search 전략
      - 단순히 첫번째 단어를 argmax로 선택하면 1스텝에서라도 문법상 실수를 할 경우, 전체 문장의 번역에 큰 실수가 되므로, 각각의 타임스텝 t마다 b개의 sequence 후보 군을 유지
  - Prompt Engineering
    - 주요 요소
      - 명확성과 구체성
      - 문맥 이해
      - 정확한 목적 전달
      - 반복과 조정
    - Prompting and Completion
      - Prompt를 model에 넣어 completion(답변, 출력 텍스트)을 생성함
    - In-Context Learning
      - zero shot prompting
        - 입력 데이터 없이 답변 생성
        - 원하는 답이 아닐 가능성이 높음
      - one shot prompting
        - 모델에게 특정 작업이나 개념에 대해, 자신이 원하는 한가지 예를 instruction과 함께 제시
        - 제공된 예와 유사한 새로운 데이터를 생성 혹은 분류


[2일차 학습 내용 링크](https://www.notion.so/AI-12d5acce602e807a89c2cbb7577a4b81?pvs=4)